% \chapter{Evaluation}

\section{Evaluation Objectives}

The evaluation phase is designed not only to measure phishing detection performance, but also to validate the correctness of the system design and to derive appropriate weights for the scoring criteria.
Rather than assigning weights subjectively, this evaluation aims to provide data-driven evidence for weight selection.

Specifically, the evaluation focuses on two objectives:
\begin{itemize}
    \item Verifying whether the selected criteria are suitable for detecting real-world phishing websites.
    \item Determining whether the assigned weights accurately reflect the actual contribution of each criterion to phishing detection.
\end{itemize}

\section{Dataset Description}

The evaluation was conducted on a dataset of 140 URLs, consisting of:
\begin{itemize}
    \item 70 phishing websites
    \item 70 legitimate websites
\end{itemize}

Phishing URLs were collected from \textit{OpenPhish}, a well-established public phishing database.
Legitimate websites were selected from trusted and commonly accessed domains.
The dataset was balanced to ensure fair comparison and unbiased evaluation.

\section{Evaluation Method 1: Group-Based Heuristic Analysis}

\subsection{Initial Weight Assignment for Component Observation}

Before deriving the final scoring weights, the system must be executed to obtain individual component scores for each URL.
This requires assigning an initial set of temporary weights.

These initial weights are not intended to represent optimal or final values.
Instead, they serve as a neutral starting point that allows the system to generate component-level outputs for analysis.
The weights were assigned in a simple and intuitive manner, ensuring that no single criterion was excessively emphasized.

Importantly, the ranking-based evaluation in Method 1 focuses on analyzing individual component scores rather than the final aggregated trust score.
Therefore, the qualitative behavior of each criterion is largely independent of the specific initial weight values.

This initial configuration enables observation of how each criterion responds to different phishing techniques, while avoiding premature optimization.
Final weight determination is performed only after quantitative validation through the ablation study.


\subsection{Group-Based Component Score Analysis}

To strengthen the ranking-based evaluation with quantitative evidence, average component scores were computed for each group.
All scores are normalized on a 0--10 scale, where higher values indicate higher perceived trustworthiness.
These results provide insight into how each criterion behaves under different phishing techniques.

\subsubsection{Group 1: Domain-Based Phishing}

\begin{table}[H]
\centering
\caption{Average Component Scores for Group 1 -- Domain-Based Phishing}
\label{tab:g1-scores}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Average Score (0--10)} \\ \hline
Certificate Details & 8.24 \\ \hline
Server Reliability & 4.63 \\ \hline
Domain Age & 8.25 \\ \hline
Domain Pattern & 8.76 \\ \hline
HTML Content and Behavior & 6.95 \\ \hline
Protocol Security & 8.57 \\ \hline
AI Analysis & 5.24 \\ \hline
Reputation Database & 1.14 \\ \hline
User Review & 0.00 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Group 2: Infrastructure and Protocol-Based Phishing}

\begin{table}[H]
\centering
\caption{Average Component Scores for Group 2 -- Infrastructure and Protocol Phishing}
\label{tab:g2-scores}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Average Score (0--10)} \\ \hline
Certificate Details & 2.65 \\ \hline
Server Reliability & 5.71 \\ \hline
Domain Age & 3.70 \\ \hline
Domain Pattern & 8.60 \\ \hline
HTML Content and Behavior & 3.66 \\ \hline
Protocol Security & 3.95 \\ \hline
AI Analysis & 4.75 \\ \hline
Reputation Database & 0.00 \\ \hline
User Review & 0.00 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Group 3: Content-Based Phishing}

\begin{table}[H]
\centering
\caption{Average Component Scores for Group 3 -- Content-Based Phishing}
\label{tab:g3-scores}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Average Score (0--10)} \\ \hline
Certificate Details & 5.45 \\ \hline
Server Reliability & 9.00 \\ \hline
Domain Age & 5.50 \\ \hline
Domain Pattern & 8.65 \\ \hline
HTML Content and Behavior & 4.64 \\ \hline
Protocol Security & 7.85 \\ \hline
AI Analysis & 4.85 \\ \hline
Reputation Database & 0.00 \\ \hline
User Review & 0.00 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Group 4: Miscellaneous Phishing}

\begin{table}[H]
\centering
\caption{Average Component Scores for Group 4 -- Miscellaneous Phishing}
\label{tab:g4-scores}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Average Score (0--10)} \\ \hline
Certificate Details & 7.90 \\ \hline
Server Reliability & 9.00 \\ \hline
Domain Age & 8.10 \\ \hline
Domain Pattern & 9.40 \\ \hline
HTML Content and Behavior & 6.30 \\ \hline
Protocol Security & 8.90 \\ \hline
AI Analysis & 5.00 \\ \hline
Reputation Database & 1.20 \\ \hline
User Review & 0.00 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Group 5: Legitimate Websites}

\begin{table}[H]
\centering
\caption{Average Component Scores for Group 5 -- Legitimate Websites}
\label{tab:g5-scores}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Average Score (0--10)} \\ \hline
Certificate Details & 9.07 \\ \hline
Server Reliability & 9.13 \\ \hline
Domain Age & 9.14 \\ \hline
Domain Pattern & 9.90 \\ \hline
HTML Content and Behavior & 7.13 \\ \hline
Protocol Security & 9.81 \\ \hline
AI Analysis & 5.00 \\ \hline
Reputation Database & 10.00 \\ \hline
User Review & 0.00 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Discussion}

These group-level component scores explain the initial ranking-based weight assignment.
Criteria that consistently produce high scores across phishing-heavy groups indicate weak detection capability and are assigned lower weights.
Conversely, criteria that yield low scores for phishing groups and high scores for legitimate websites are prioritized.
However, since this method reflects average group behavior rather than direct contribution to classification performance, it is complemented by an ablation study for quantitative validation.



\subsection{Heuristic Weight Adjustment Logic}

The following heuristic logic was applied:
\begin{itemize}
    \item If a criterion assigns high scores to URLs that are known to be phishing, it indicates weak phishing detection capability and its weight should be reduced.
    \item If a criterion consistently assigns low scores to phishing-heavy groups, it indicates strong phishing detection capability and its weight should be increased.
\end{itemize}

Using this approach, an initial set of scoring weights was derived.

\subsection{Ranking-Based Weight Configuration}

Table~\ref{tab:ranking-weights} presents the scoring weights obtained from the ranking-based evaluation method.

\begin{table}[H]
\centering
\caption{Scoring Weights Derived from Ranking-Based Evaluation}
\label{tab:ranking-weights}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Score Weight} \\ \hline
Certificate Details & 0.6 \\ \hline
Server Reliability & 0.4 \\ \hline
Domain Age & 0.7 \\ \hline
Domain Pattern & 0.7 \\ \hline
HTML Content and Behavior & 1.1 \\ \hline
Protocol Security & 0.7 \\ \hline
AI Analysis & 1.0 \\ \hline
Reputation Database & 1.6 \\ \hline
User Review & 0.2 \\ \hline
\end{tabular}
\end{table}

While this configuration reflects observed group-level behavior, it is derived from heuristic reasoning rather than direct quantitative contribution measurement.

\section{Evaluation Method 2: Ablation Study}

\subsection{Baseline Configuration}

To quantitatively measure criterion contribution, an ablation study was conducted.
The dataset was simplified into two balanced groups:
\begin{itemize}
    \item 70 phishing websites
    \item 70 legitimate websites
\end{itemize}

All criteria were initially assigned equal weights of 1.0 to form a neutral baseline.
The baseline average trust scores were:
\begin{itemize}
    \item Phishing websites: 2.741 / 5.0
    \item Legitimate websites: 3.896 / 5.0
\end{itemize}

\subsection{Ablation Procedure}

Each criterion was removed individually by setting its weight to zero while keeping all subscores unchanged.
The trust score was then recomputed offline without rerunning the analysis modules.

For each ablation step, the average score change was recorded separately for phishing websites and legitimate websites.

\subsection{Detailed Ablation Results}

To provide a clearer quantitative view of the ablation study, detailed results for both legitimate and phishing website groups are presented in Tables~\ref{tab:ablation-legit} and~\ref{tab:ablation-phishing}.
Each table reports the ablated average score, score drop, and contribution percentage for each criterion.

\begin{table}[H]
\centering
\caption{Ablation Results for Legitimate Websites (Baseline Average Score: 3.896 / 5.0)}
\label{tab:ablation-legit}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Criterion} & \textbf{Ablated Score} & \textbf{Score Drop} & \textbf{Contribution} \\ \hline
Protocol Security & 3.760 & 0.136 & 3.49\% \\ \hline
Server Reliability & 3.772 & 0.125 & 3.20\% \\ \hline
Domain Pattern & 3.762 & 0.134 & 3.45\% \\ \hline
HTML Content and Behavior & 3.846 & 0.051 & 1.30\% \\ \hline
Certificate Details & 3.853 & 0.043 & 1.11\% \\ \hline
Domain Age & 3.803 & 0.093 & 2.39\% \\ \hline
AI Analysis & 4.116 & -0.219 & -5.63\% \\ \hline
Reputation Database & 3.773 & 0.124 & 3.17\% \\ \hline
User Review & 4.383 & -0.487 & -12.50\% \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Ablation Results for Phishing Websites (Baseline Average Score: 2.741 / 5.0)}
\label{tab:ablation-phishing}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Criterion} & \textbf{Ablated Score} & \textbf{Score Drop} & \textbf{Contribution} \\ \hline
Protocol Security & 2.523 & 0.218 & 7.95\% \\ \hline
Server Reliability & 2.553 & 0.188 & 6.87\% \\ \hline
Domain Pattern & 2.557 & 0.184 & 6.70\% \\ \hline
HTML Content and Behavior & 2.583 & 0.158 & 5.76\% \\ \hline
Certificate Details & 2.699 & 0.041 & 1.51\% \\ \hline
Domain Age & 2.776 & -0.035 & -1.29\% \\ \hline
AI Analysis & 2.831 & -0.091 & -3.30\% \\ \hline
Reputation Database & 3.061 & -0.320 & -11.69\% \\ \hline
User Review & 3.083 & -0.343 & -12.50\% \\ \hline
\end{tabular}
\end{table}


\subsection{Ablation-Based Weight Configuration}

Based on the ablation results, scoring weights were redistributed according to the measured contribution of each criterion.

Table~\ref{tab:ablation-weights} presents the scoring weights derived from the ablation study.

\begin{table}[H]
\centering
\caption{Scoring Weights Derived from Ablation Study}
\label{tab:ablation-weights}
\begin{tabular}{|l|c|}
\hline
\textbf{Criterion} & \textbf{Score Weight} \\ \hline
Certificate Details & 0.6 \\ \hline
Server Reliability & 0.8 \\ \hline
Domain Age & 1.0 \\ \hline
Domain Pattern & 0.8 \\ \hline
HTML Content and Behavior & 0.7 \\ \hline
Protocol Security & 0.8 \\ \hline
AI Analysis & 1.5 \\ \hline
Reputation Database & 2.0 \\ \hline
User Review & 0.1 \\ \hline
\end{tabular}
\end{table}

The ablation-based configuration emphasizes criteria that demonstrate strong quantitative impact on phishing detection, while reducing the influence of unstable or low-impact criteria.

\section{Comparison and Final Weight Selection}

Figure~\ref{fig:weight-comparison} provides a visual comparison between the scoring weights derived from the ranking-based evaluation and the ablation study.
Instead of relying solely on textual comparison, this visualization highlights how each method distributes importance across the scoring criteria.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Images/weight_comparison.png}
    \caption{Visual Comparison of Scoring Weights Between Ranking-Based and Ablation-Based Methods}
    \label{fig:weight-comparison}
\end{figure}

As shown in Figure~\ref{fig:weight-comparison}, the two evaluation methods produce noticeably different weight distributions.
The ranking-based method emphasizes criteria based on observed group behavior, such as HTML Content and Behavior.
In contrast, the ablation study reallocates weights according to measured score impact on classification performance.

Notably, AI Analysis and Reputation Database receive substantially higher weights under the ablation study, reflecting their strong quantitative contribution to phishing detection.
Meanwhile, User Review remains consistently low due to insufficient and unstable data.

Based on this clear quantitative and visual comparison, the scoring weights derived from the ablation study are selected as the final configuration.
This evaluation-driven approach ensures that the final scoring system is data-driven, explainable, and robust for real-world phishing detection.

\section{Performance Evaluation}

In addition to detection correctness and weight validation, system performance was evaluated to assess practical usability.
The evaluation focuses on per-URL processing time, which directly reflects real-world user experience.

\subsection{Measurement Scope}

Execution time was measured for each URL processed by the system.
The recorded runtime includes the complete analysis pipeline, consisting of:
\begin{itemize}
    \item Network request and response handling
    \item Certificate and protocol inspection
    \item Domain and server analysis
    \item HTML content parsing
    \item AI-based content analysis
    \item Reputation database lookup
\end{itemize}

All measurements were conducted under identical conditions, and runtime data was logged for each URL across all five evaluation groups.

\subsection{Runtime Summary}

Table~\ref{tab:runtime-summary} reports the average per-URL processing time across different evaluation groups.
The average execution time reflects typical system performance during normal usage.

\begin{table}[H]
\centering
\caption{Average Runtime per URL Across Evaluation Groups}
\label{tab:runtime-summary}
\begin{tabular}{|l|c|}
\hline
\textbf{Group} & \textbf{Average Time (ms)} \\ \hline
Group 1: Domain-based Phishing & 14,380 \\ \hline
Group 2: Infrastructure-based Phishing & 15,250 \\ \hline
Group 3: Content-based Phishing & 13,830 \\ \hline
Group 4: Miscellaneous Phishing & 10,760 \\ \hline
Group 5: Legitimate Websites & 11,000 \\ \hline
\end{tabular}
\end{table}


\subsection{Discussion}

The results show that average per-URL processing time remains within a consistent range across all evaluation groups.
No significant runtime degradation is observed for any specific phishing category.

Although the system performs multi-layer analysis, including AI-based detection and reputation database lookup, the runtime remains stable.
This confirms that the application is suitable for interactive and real-world phishing detection scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional Evaluation: Threshold Calibration and Validation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Trust Score Threshold Calibration}

In addition to scoring weight determination, the evaluation also investigates the effectiveness of the trust score thresholds used to classify websites into safety categories.
Initially, the trust score was mapped to three safety levels based on references from existing tools combined with expert judgment:

\begin{itemize}
    \item \textbf{Below 2.5:} Unsafe
    \item \textbf{2.5--4.0:} Caution
    \item \textbf{Above 4.0:} Trusted
\end{itemize}

To validate this initial hypothesis, a calibration experiment was conducted using a dataset of 20 legitimate URLs and 20 phishing URLs collected from OpenPhish.
Statistical analysis shows that all phishing URLs received trust scores below 3.58, with approximately 90\% of them scoring under 3.0.
In contrast, all legitimate URLs achieved trust scores above 4.5, indicating a clear separation between malicious and benign websites.

Based on these observations, the primary optimization objective was to prevent high-confidence phishing URLs from being classified as \textit{Trusted} or \textit{Caution}, while minimizing the risk of incorrectly labeling legitimate websites as unsafe.
Accordingly, the trust score thresholds were refined as follows:

\begin{itemize}
    \item \textbf{Below 3.0:} Unsafe
    \item \textbf{3.0--4.0:} Caution
    \item \textbf{Above 4.0:} Trusted
\end{itemize}

Although the calibration results suggest that the \textit{Trusted} threshold could be increased to 4.5, the final threshold was intentionally maintained at 4.0.
This decision is motivated by two considerations.
First, setting thresholds too close to the calibration data may lead to overfitting and reduce generalization to unseen websites.
Second, the interval between 4.0 and 4.5 provides a practical safety margin that accommodates legitimate websites with minor technical imperfections, thereby reducing false positives in real-world usage.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Threshold Impact Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Impact of Threshold Adjustment}

To quantify the effect of threshold calibration, both the original and revised threshold configurations were applied to a dataset of 90 phishing URLs used in the benchmarking phase.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/threshold_impact.png}
    \caption{Impact of Threshold Adjustment on Website Classification Outcomes}
    \label{fig:threshold-impact}
\end{figure}

The results indicate that, after threshold adjustment, the proportion of URLs classified as \textit{Unsafe} increased by approximately 40\%, while the number of URLs classified as \textit{Caution} decreased by a similar margin.
This reduction of the ambiguous \textit{Caution} zone leads to more decisive and actionable warnings for end users, without increasing false positives.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Comparative Benchmarking
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Comparative Benchmarking with Existing Tools}

To assess real-world effectiveness, TrueWeb was benchmarked against several popular website safety tools commonly used by general Internet users.
The benchmark dataset consists of 30 legitimate URLs and 90 phishing URLs, distinct from those used in threshold calibration.

The comparison tools include Chongluadao.vn, PhishTank, ScamAdvisor, Norton Safe Web, VirusTotal, Bitdefender, Skynag, and Web of Trust.
Each tool was evaluated according to its native scoring or classification criteria to ensure fairness.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{Images/tool_comparison_accuracy.png}
    \caption{Detection Accuracy Comparison Between TrueWeb and Existing Tools}
    \label{fig:tool-comparison}
\end{figure}

TrueWeb successfully processed all active websites, achieving a 100\% availability rate.
For legitimate websites, TrueWeb correctly classified all 30 URLs as safe.
For phishing websites, TrueWeb detected 70 out of 90 URLs; after excluding 12 inactive links, the effective detection accuracy reached approximately 89.74\%.
Overall, the benchmark confirms that TrueWeb achieves competitive performance comparable to existing commercial tools, particularly for newly created phishing websites that are often missed by blacklist-based solutions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User Experience Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{User Experience Evaluation}

Beyond detection performance, a user experience (UX) survey was conducted to evaluate usability and perceived effectiveness.
The survey followed a three-step process: questionnaire design, controlled deployment with a user manual, and response collection.

A total of 50 participants took part in the study.
Participants rated multiple criteria on a 5-point Likert scale, including UI/UX, response speed, background operation, result presentation, and the review feature.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/ux_score_distribution.png}
    \caption{Distribution of User Satisfaction Scores}
    \label{fig:ux-distribution}
\end{figure}

The overall average satisfaction score was 4.34 out of 5.
Positive ratings (4--5 points) accounted for 87.6\% of responses, while perfect ratings (5 points) accounted for 54.4\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Images/ux_criteria_scores.png}
    \caption{Average User Ratings by Evaluation Criterion}
    \label{fig:ux-criteria}
\end{figure}

The highest-rated feature was the \textit{View and Write Reviews} module, while response speed received the lowest score.
Nevertheless, the difference between criteria was minimal, indicating consistent user satisfaction across features.
Despite limitations in sample size and demographic diversity, the survey confirms that TrueWeb meets core user needs and provides a positive user experience suitable for a prototype-stage deployment.






